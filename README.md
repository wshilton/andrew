# sensorfusion

The ultimate aim of this work is to develop a prototype platform for detecting, tracking, and classifying fully context-based social data. The platform is envisioned to provide a unified, multi-domain sensor API for a social regulation framework, which integrates sensor input into social primitives with various cross-domain text, image, and audio neural networks. 

For instance, a catalog of unique facial identities may be readily constructed, following some post-processing, from a linear synthetic discriminant applied to optical input. Parallel infrared processing on regions with active identities can aid the optical data for purposes of generating a classification of emotional states for the identity. Likewise, acoustic data from a beamsteered uniform circular and uniform planar acoustic array may be integrated with the optical and infrared data for acquiring an acoustic signal that identifies vocal utterances, along with, perhaps, facilities for validating and verifying the acoustic signature for purposes of identity management. Use of a uniform planar microwave array is envisioned for motion detection, which aids time-series identity management -- both alerting process managers to initiate finer optical-based facial recognition and aiding optical tracking algorithms for moving-target processing. Angle-of-arrival capabilities in the uniform cicruclar array will also aid in scheduling based on the detection of certain acoustic signals. Spatial awareness is gathered from an inertial sensor.

Once all of the relevant social primitives have been constructed for all signals from a given context, the platform will have established a condition of social awareness for a small time interval described by a collection of human identities: emotional states, utterances, and body language symbols. This data is to be integrated with some (possibly large) collection of historical moments using cross-domain neural networks for purposes of generating socially relevant responses to the moment.
