{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f5a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_face_landmarks_on_image(rgb_image, detection_result):\n",
    "    face_landmarks_list = detection_result.face_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected faces to visualize.\n",
    "    for idx in range(len(face_landmarks_list)):\n",
    "        face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "        # Draw the face landmarks.\n",
    "        face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        face_landmarks_proto.landmark.extend([\n",
    "          landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "        ])\n",
    "\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "              landmark_drawing_spec=None,\n",
    "              connection_drawing_spec=mp.solutions.drawing_styles\n",
    "              .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "    # Extract the face blendshapes category names and scores.\n",
    "    face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "    face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "    # The blendshapes are ordered in decreasing score value.\n",
    "    face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "    ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Label each bar with values\n",
    "    for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "        plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title(\"Face Blendshapes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7954c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W20230820 10:40:33.254999 685022 face_landmarker_graph.cc:168] Face blendshape model contains CPU only ops. Sets FaceBlendshapesGraph acceleration to Xnnpack.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'tuple' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#Zoom frame\u001b[39;00m\n\u001b[1;32m     47\u001b[0m zoomed_frame \u001b[38;5;241m=\u001b[39m scp\u001b[38;5;241m.\u001b[39mndimage\u001b[38;5;241m.\u001b[39mzoom(frame, \u001b[38;5;241m1.25\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m begin_idxs \u001b[38;5;241m=\u001b[39m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzoomed_frame\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvid_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvid_height\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     49\u001b[0m end_idxs \u001b[38;5;241m=\u001b[39m begin_idxs \u001b[38;5;241m+\u001b[39m [vid_width, vid_height]\n\u001b[1;32m     51\u001b[0m frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtake(zoomed_image, np\u001b[38;5;241m.\u001b[39marange(begin_idxs,end_idxs))\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'tuple' and 'list'"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "import time\n",
    "import scipy as scp\n",
    "\n",
    "vid_width = 1280\n",
    "vid_height = 720\n",
    "\n",
    "vid = cv2.VideoCapture('./raw_training_data/1/1.mp4')\n",
    "codec = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "vid.set(6, codec)\n",
    "vid.set(5, 30)\n",
    "vid.set(3, vid_width)\n",
    "vid.set(4, vid_height)\n",
    "out = cv2.VideoWriter('./raw_training_data/1/output.avi',codec, 10, (vid_width,vid_height))\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "FaceLandmarker = mp.tasks.vision.FaceLandmarker\n",
    "FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions\n",
    "FaceLandmarkerResult = mp.tasks.vision.FaceLandmarkerResult\n",
    "\n",
    "def update_face_buffer(result: FaceLandmarkerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    landmarkbuffer.add_faces(result)\n",
    "\n",
    "face_options = FaceLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='face_landmarker.task'),\n",
    "    running_mode=VisionRunningMode.VIDEO,\n",
    "    output_face_blendshapes=True,\n",
    "    output_facial_transformation_matrixes=False,\n",
    "    num_faces=2,\n",
    "    min_face_detection_confidence=0.1,\n",
    "    min_face_presence_confidence=0.1,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "with FaceLandmarker.create_from_options(face_options) as facelandmarker:\n",
    "    while (vid.isOpened()):\n",
    "        \n",
    "        #Read frame\n",
    "        ret, frame = vid.read()\n",
    "        frame_timestamp_ms = round(time.time()*1000)\n",
    "        \n",
    "        #Zoom frame\n",
    "        scaled_dim_lengths = np.int32(np.floor(np.divide(np.shape(frame)[0:2],1.25)))\n",
    "        begin_idxs = np.int32(np.ceil(np.subtract(np.divide(np.shape(frame)[0:2],2),np.divide(np.shape(frame)[0:2],2))))\n",
    "        end_idxs = np.add(begin_idxs,scaled_dim_lengths)\n",
    "        width_idxs = np.arange(begin_idxs[0],end_idxs[0])\n",
    "        height_idxs = np.arange(begin_idxs[1],end_idxs[1])\n",
    "        cropped = frame[width_idxs, height_idxs]\n",
    "        np.take(frame, np.arange(begin_idxs[0],end_idxs[0]))\n",
    "        zoomed_frame_ch1 = scp.ndimage.zoom(frame[:,:,1], 1.25)\n",
    "        begin_idxs = (np.shape(zoomed_frame)-[vid_width, vid_height])/2\n",
    "        end_idxs = begin_idxs + [vid_width, vid_height]\n",
    "        \n",
    "        frame = np.take(zoomed_image, np.arange(begin_idxs,end_idxs))\n",
    "        \n",
    "        #Load the input image.\n",
    "        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "\n",
    "        #Detect face landmarks from the video.\n",
    "        face_landmarker_result = facelandmarker.detect_for_video(image, frame_timestamp_ms)\n",
    "\n",
    "        #Annotate frame\n",
    "        annotated_image = draw_face_landmarks_on_image(image.numpy_view(), face_landmarker_result)\n",
    "\n",
    "        #Display frame\n",
    "        cv2.imshow(\"Face landmarks\", annotated_image)\n",
    "        out.write(annotated_image)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF==ord('q'): # quit when 'q' is pressed\n",
    "            vid.release()\n",
    "            break\n",
    "\n",
    "out.release()\n",
    "cv2.destroyWindow(\"Face landmarks\") \n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c6dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andrew",
   "language": "python",
   "name": "andrew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
