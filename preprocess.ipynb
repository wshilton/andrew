{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f5a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_face_landmarks_on_image(rgb_image, detection_result):\n",
    "    face_landmarks_list = detection_result.face_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected faces to visualize.\n",
    "    for idx in range(len(face_landmarks_list)):\n",
    "        face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "        # Draw the face landmarks.\n",
    "        face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        face_landmarks_proto.landmark.extend([\n",
    "          landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "        ])\n",
    "\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "              landmark_drawing_spec=None,\n",
    "              connection_drawing_spec=mp.solutions.drawing_styles\n",
    "              .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "    # Extract the face blendshapes category names and scores.\n",
    "    face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "    face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "    # The blendshapes are ordered in decreasing score value.\n",
    "    face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "    ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Label each bar with values\n",
    "    for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "        plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title(\"Face Blendshapes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7954c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W20230821 12:46:10.974512 841203 face_landmarker_graph.cc:168] Face blendshape model contains CPU only ops. Sets FaceBlendshapesGraph acceleration to Xnnpack.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "import time\n",
    "import scipy as scp\n",
    "\n",
    "vid_width = 1280\n",
    "vid_height = 720\n",
    "\n",
    "vid = cv2.VideoCapture('./raw_training_data/1/1.mp4')\n",
    "codec = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "vid.set(6, codec)\n",
    "vid.set(5, 30)\n",
    "vid.set(3, vid_width)\n",
    "vid.set(4, vid_height)\n",
    "out = cv2.VideoWriter('./raw_training_data/1/output.avi',codec, 10, (vid_width,vid_height))\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "FaceLandmarker = mp.tasks.vision.FaceLandmarker\n",
    "FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions\n",
    "FaceLandmarkerResult = mp.tasks.vision.FaceLandmarkerResult\n",
    "\n",
    "face_options = FaceLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='face_landmarker.task'),\n",
    "    running_mode=VisionRunningMode.VIDEO,\n",
    "    output_face_blendshapes=True,\n",
    "    output_facial_transformation_matrixes=False,\n",
    "    num_faces=2,\n",
    "    min_face_detection_confidence=0.5,\n",
    "    min_face_presence_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "with FaceLandmarker.create_from_options(face_options) as facelandmarker:\n",
    "        while (vid.isOpened()):\n",
    "\n",
    "            #Read frame\n",
    "            ret, frame = vid.read()\n",
    "            frame_timestamp_ms = round(time.time()*1000)\n",
    "            \n",
    "            #Detect faces with Haar cascade\n",
    "            faces = face_classifier.detectMultiScale(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), 1.1, 10, minSize=(80, 80))\n",
    "            \n",
    "            #Add old faces according to face tracker\n",
    "            \n",
    "            #Zoom faces\n",
    "            for (x, y, w, h) in faces:\n",
    "                lg_w = int( 1.4 * w)\n",
    "                lg_h = int(16.  * w / 9)\n",
    "                x = int(x - abs(lg_w - w)/2.0)\n",
    "                y = int(y - abs(lg_h - h)/2.0)\n",
    "                frame = frame[x:(x + lg_w),y:(y + lg_h),:]\n",
    "                scaled_dim_lengths = np.int32(np.floor(np.divide(np.shape(frame)[0:2],1.25)))\n",
    "                begin_idxs = np.int32(np.ceil(np.subtract(np.divide(np.shape(frame)[0:2],2),np.divide(np.shape(frame)[0:2],2))))\n",
    "                end_idxs = np.add(begin_idxs,scaled_dim_lengths)\n",
    "                width_idxs = np.arange(begin_idxs[0],end_idxs[0])\n",
    "                height_idxs = np.arange(begin_idxs[1],end_idxs[1])\n",
    "                cropped = frame[width_idxs, height_idxs]\n",
    "                np.take(frame, np.arange(begin_idxs[0],end_idxs[0]))\n",
    "                zoomed_frame_ch1 = scp.ndimage.zoom(frame[:,:,1], 1.25)\n",
    "                begin_idxs = (np.shape(zoomed_frame)-[vid_width, vid_height])/2\n",
    "                end_idxs = begin_idxs + [vid_width, vid_height]\n",
    "                #frame = np.take(zoomed_image, np.arange(begin_idxs,end_idxs))\n",
    "\n",
    "                #Detect face landmarks from each zoomed face.\n",
    "                face_landmarker_result = facelandmarker.detect_for_video(mp.Image(image_format=mp.ImageFormat.SRGB, data=frame), frame_timestamp_ms)\n",
    "            \n",
    "            #Annotate frame with landmarks\n",
    "            annotated_image = draw_face_landmarks_on_image(image.numpy_view(), face_landmarker_result)\n",
    "            \n",
    "            #Annotate frame with face bounding boxes\n",
    "            for (x, y, w, h) in faces:\n",
    "                lg_w = int( 1.4 * w)\n",
    "                lg_h = int(16.  * w / 9)\n",
    "                x = int(x - abs(lg_w - w)/2.0)\n",
    "                y = int(y - abs(lg_h - h)/2.0)\n",
    "                cv2.rectangle(annotated_image, (x, y), (x + lg_w, y + lg_h), (0, 255, 0), 4)\n",
    "\n",
    "            #Display frame\n",
    "            cv2.imshow(\"Face landmarks\", annotated_image)\n",
    "            out.write(annotated_image)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF==ord('q'): # quit when 'q' is pressed\n",
    "                vid.release()\n",
    "                break\n",
    "\n",
    "out.release()\n",
    "cv2.destroyWindow(\"Face landmarks\") \n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c6dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andrew",
   "language": "python",
   "name": "andrew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
